**Argument Diagrams and Improved Argument Assessment**

In the 2021-22 academic year, I implemented a major curriculum change - throughgoing incorporation of argument diagramming exercises (using 
MindMup: https://www.mindmup.com/tutorials/argument-visualization.html) - in a class that I was teaching on scientific reasoning, evidence, and argumentation.

I was inspired to make this curricular change, in part, by empirical results I'd seen from a study carried out in Princeton University introductory philosophy 
classes: https://www.nature.com/articles/s41539-018-0038-5. I decided to gather similar data in my own class to see if the results were similar in a somewhat 
different setting (9th-graders vs. college freshmen, a class focused on scientific argumentation rather than philosophical argumentation, year-long vs. semester 
class, online vs. in-person, etc.). I was not able to run this as an experiment or quasi-experiment (as in the Princeton study), but I thought the results in a 
new setting could still be a useful supplement.

As in the Princeton study, the main measurable outcome was performance on the LSAT Logical Reasoning section. Students took one such section at the beginning of 
the year and a different section at the end of the year (with standard time limits, unless students had school-sanctioned accommodations, and with the order of 
the two sections randomized).

The results for my class were in line with those seen in the Princeton study: on average, students answered substantially more questions correctly at the end of 
the year than they did at the beginning. More specifically, depending on how the analysis was done (see below), their scores improved by 1.6 to 2.1 correct answers 
out of 26 questions. This improvement was highly statistically significant (P=0.0002) and the equivalent of moving from the ~31st to ~45th percentile of LSAT takers.

Notebook: https://github.com/jsbeattie1/Argument_Assessment_Improvement/blob/main/Argument%20Assessment%20Improvement.ipynb
